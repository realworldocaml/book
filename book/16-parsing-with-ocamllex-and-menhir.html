    <section id="parsing-with-ocamllex-and-menhir" data-type="chapter">
      <h1>Parsing with OCamllex and Menhir</h1>

      <p>
	Many programming tasks start with the interpretion of some
	form of structured textual data. <em>Parsing</em> is the
	process of converting such data into data structures that are
	easy to program against. For simple formats, it's often enough
	to parse the data in an ad hoc way, say, by breaking up the
	data into lines, and then using regular expressions for
	breaking those lines down into their component pieces.</p>

      <p>
	But this simplistic approach tends to fall down when parsing
	more complicated data, particularly data with the kind of
	recursive structure you find in full-blown programming
	languages or flexible data formats like JSON and XML. Parsing
	such formats accurately and efficiently while providing useful
	error messages is a complex task.</p>

      <p>
	Often, you can find an existing parsing library that handles
	these issues for you. But there are tools to simplify the task
	when you do need to write a parser, in the form of <em>parser
	  generators</em>. A parser generator creates a parser from a
	specification of the data format that you want to parse, and
	uses that to generate a parser.

	<idx>parsing/parser generators</idx></p>

      <p>

	Parser generators have a long history, including tools
	like <span class="command"><em>lex</em></span>
	and <span class="command"><em>yacc</em></span> that date back to
	the early 1970s. OCaml has its own alternatives,
	including <span class="command"><em>ocamllex</em></span>, which
	replaces <span class="command"><em>lex</em></span>,
	and <span class="command"><em>ocamlyacc</em></span>
	and <span class="command"><em>menhir</em></span>, which
	replace <span class="command"><em>yacc</em></span>. We'll
	explore these tools in the course of walking through the
	implementation of a parser for the JSON serialization format
	that we discussed in <a href="15-json.html#handling-json-data" data-type="xref">Handling
	  Json Data</a>.</p>

      <p>
	Parsing is a broad and often intricate topic, and our purpose
	here is not to teach all of the theoretical issues, but to provide
	a pragmatic introduction of how to build a parser in
	OCaml.

	<idx>ocamlyacc parser generator</idx>
	<idx>Menhir parser generator/vs. ocamlyacc</idx></p>

      <div data-type="note">
	<h1>Menhir Versus ocamlyacc</h1>

	<p>
	  Menhir is an alternative parser generator that is generally
	  superior to the
	  venerable <span class="command"><em>ocamlyacc</em></span>,
	  which dates back quite a few years. Menhir is mostly
	  compatible
	  with <span class="command"><em>ocamlyacc</em></span>
	  grammars, and so you can usually just switch to Menhir and
	  expect older code to work (with some minor differences
	  described in the Menhir manual).</p>

	<p>
	  The biggest advantage of Menhir is that its error messages
	  are generally more human-comprehensible, and the parsers
	  that it generates are fully reentrant and can be
	  parameterized in OCaml modules more easily. We recommend
	  that any new code you develop should use Menhir instead
	  of <span class="command"><em>ocamlyacc</em></span>.</p>

	<p>
	  Menhir isn't distributed directly with OCaml but is
	  available through OPAM by running <code>opam install
	    menhir</code>.</p>
      </div>

      <section id="lexing-and-parsing" data-type="sect1">
	<h1>Lexing and Parsing</h1>

	<p>
	  Parsing is traditionally broken down into two parts:
	  <em>lexical analysis</em>, which is a kind of simplified
	  parsing phase that converts a stream of characters into a
	  stream of logical tokens; and full-on parsing, which involves
	  converting a stream of tokens into the final representation,
	  which is often in the form of a tree-like data structure
	  called an <em>abstract syntax tree</em>, or AST.

	  <idx>AST (abstract syntax-tree)</idx>
	  <idx>lexical analysis (lexing)</idx></p>

	<p>
	  It's confusing that the term parsing is applied to both
	  the overall process of converting textual data to structured
	  data, and also more specifically to the second phase of
	  converting a stream of tokens to an AST; so from here on out,
	  we'll use the term parsing to refer only to this second
	  phase.</p>

	<p>
	  Let's consider lexing and parsing in the context of the
	  JSON format. Here's a snippet of text that represents a JSON
	  object containing a string labeled <code>title</code> and an
	  array containing two objects, each with a name and array of
	  zip codes:</p>

	<link rel="import" href="code/parsing/example.json"/>

	<p>
	  At a syntactic level, we can think of a JSON file as a
	  series of simple logical units, like curly braces, square
	  brackets, commas, colons, identifiers, numbers, and quoted
	  strings. Thus, we could represent our JSON text as a sequence
	  of tokens of the following type:</p>

	<link rel="import" href="code/parsing/manual_token_type.ml"/>

	<p>
	  Note that this representation loses some information about
	  the original text. For example, whitespace is not
	  represented. It's common, and indeed useful, for the token
	  stream to forget some details of the original text that are
	  not required for understanding its meaning.</p>

	<p>
	  If we converted the preceding example into a list of these
	  tokens, it would look something like this:</p>
	<link rel="import" href="code/parsing/tokens.ml"/>

	<p>
	  This kind of representation is easier to work with than the
	  original text, since it gets rid of some unimportant
	  syntactic details and adds useful structure. But it's still
	  a good deal more low-level than the simple AST we used for
	  representing JSON data
	  in <a href="15-json.html#handling-json-data" data-type="xref">Handling
	    Json Data</a>:</p>

	<link rel="import" href="code/parsing/json.ml"/>

	<p>
	  This representation is much richer than our token stream,
	  capturing the fact that JSON values can be nested inside each
	  other and that JSON has a variety of value types, including
	  numbers, strings, arrays, and objects. The parser we'll write
	  will convert a token stream into a value of this AST type, as
	  shown below for our earlier JSON example:</p>
	<link rel="import" href="code/parsing/parsed_example.ml"/>
      </section>

      <section id="defining-a-parser" data-type="sect1">
	<h1>Defining a Parser</h1>

	<p>
	  A parser-specification file has suffix <code>.mly</code>
	  and contains two sections that are broken up by separator
	  lines consisting of the characters <code>%%</code> on a line
	  by themselves. The first section of the file is for
	  declarations, including token and type specifications,
	  precedence directives, and other output directives; and the
	  second section is for specifying the grammar of the language
	  to be parsed.

	  <idx>files/mly files</idx>
	  <idx id="PARSparsdef">parsing/parser definition</idx></p>

	<p>
	  We'll start by declaring the list of tokens. A token is
	  declared using the syntax <code>%token
	    &lt;</code><em><code>type</code></em><code>&gt;</code>
	  <em><code>uid</code></em>, where the
	  <em><code>&lt;type&gt;</code></em> is optional and
	  <em><code>uid</code></em> is a capitalized identifier. For
	  JSON, we need tokens for numbers, strings, identifiers, and
	  punctuation:

	  <idx>tokens, declaration of</idx></p>


	<link rel="import" href="code/parsing/parser.mly"/>

	<p>
	  The
	  <code>&lt;</code><em><code>type</code></em><code>&gt;</code>
	  specifications mean that a token carries a value. The
	  <code>INT</code> token carries an integer value with it,
	  <code>FLOAT</code> has a <code>float</code> value, and
	  <code>STRING</code> carries a <code>string</code> value. The
	  remaining tokens, such as <code>TRUE</code>,
	  <code>FALSE</code>, or the punctuation, aren't associated
	  with any value, and so we can omit the
	  <code>&lt;</code><em><code>type</code></em><code>&gt;</code>
	  specification.</p>

	<section id="describing-the-grammar" data-type="sect2">
          <h2>Describing the Grammar</h2>

          <p>
	    The next thing we need to do is to specify the grammar of
            a JSON
            expression.  <span class="command"><em>menhir</em></span>,
            like many parser generators, expresses grammars
            as <em>context-free
              grammars</em>. (More
              precisely, <span class="command"><em>menhir</em></span>
              supports LR(1) grammars,
            but we will ignore that technical distinction here.) You
            can think of a context-free grammar as a set of abstract
            names, called <em>non-terminal symbols</em>, along with a
            collection of rules for transforming a nonterminal symbol
            into a sequence of tokens and nonterminal symbols. A
            sequence of tokens is parsable by a grammar if you can
            apply the grammar's rules to produce a series of
            transformations, starting at a distinguished <em>start
              symbol</em> that produces the token sequence in
            <span class="keep-together">question</span>.

	    <idx>grammars/context-free</idx>
	    <idx>LR(1) grammars</idx>
	    <idx>start symbols</idx>
	    <idx>non-terminal symbols</idx>
	    <idx>context-free grammars</idx>
	    <idx>Menhir parser generator/context-free grammars in</idx></p>

          <p>
	    We'll start describing the JSON grammar by declaring the
            start symbol to be the non-terminal symbol
            <code>prog</code>, and by declaring that when parsed, a
            <code>prog</code> value should be converted into an OCaml
            value of type <code>Json.value option</code>. We then end
            the declaration section of the parser with a
            <code>%%</code>:</p>
          <link rel="import" href="code/parsing/parser.mly" part="1"/>

          <p>
	    Once that's in place, we can start specifying the
            productions. In <span class="command"><em>menhir</em></span>,
            productions are organized into <em>rules</em>, where each
            rule lists all the possible productions for a given
            nonterminal symbols. Here, for example, is the rule
            for <code>prog</code>:</p>

          <link rel="import" href="code/parsing/parser.mly" part="2"/>

          <p>
	    The syntax for this is reminiscent of an OCaml
            <code>match</code> statement. The pipes separate the
            individual productions, and the curly braces contain a
            <em>semantic action</em>: OCaml code that generates the
            OCaml value corresponding to the production in question.
            Semantic actions are arbitrary OCaml expressions that are
            evaluated during parsing to produce values that are
            attached to the non-terminal in the rule.

	    <idx>semantic actions</idx>
	    <idx>curly braces ({ })</idx></p>

          <p>
	    We have two cases for <code>prog</code>: either there's
            an <code>EOF</code>, which means the text is empty, and so
            there's no JSON value to read, we return the OCaml value
            <code>None</code>; or we have an instance of the
            <code>value</code> nonterminal, which corresponds to a
            well-formed JSON value, and we wrap the corresponding
            <code>Json.value</code> in a <code>Some</code> tag. Note
            that in the <code>value</code> case, we wrote <code>v =
              value</code> to bind the OCaml value that corresponds to
            the variable <code>v</code>, which we can then use within
            the curly braces for that production.</p>

          <p>
	    Now let's consider a more complex example, the rule for
            the <code>value</code> symbol:</p>
          <link rel="import" href="code/parsing/parser.mly" part="3"/>

          <p>
	    According to these rules, a JSON <code>value</code> is
            either:

	    <idx>values/in JSON data</idx></p>

          <ul>
            <li>
              <p>
		An object bracketed by curly braces</p>
            </li>

            <li>
              <p>
		An array bracketed by square braces</p>
            </li>

            <li>
              <p>
		A string, integer, float, bool, or null value</p>
            </li>
          </ul>

          <p>
	    In each of the productions, the OCaml code in curly
            braces shows what to transform the object in question to.
            Note that we still have two nonterminals whose definitions
            we depend on here but have not yet defined:
            <code>object_fields</code> and <code>array_values</code>.
            We'll look at how these are parsed next.</p>
	</section>

	<section id="parsing-sequences" data-type="sect2">
          <h2>Parsing Sequences</h2>

          <p>
	    The rule for <code>object_fields</code> follows, and is
            really just a thin wrapper that reverses the list returned
            by the following rule for <code>rev_object_fields</code>.
            Note that the first production
            in <code>rev_object_fields</code> has an empty left-hand
            side, because what we're matching on in this case is an
            empty sequence of tokens. The comment <code>(* empty
            *)</code> is used to make this clear:

	    <idx>rev_object_fields</idx>
	    <idx>object_fields</idx></p>


          <link rel="import" href="code/parsing/parser.mly" part="4"/>

          <p>
	    The rules are structured as they are because
            <span class="command"><em>menhir</em></span> generates
            left-recursive parsers, which means that the constructed
            pushdown automaton uses less stack space with
            left-recursive definitions. The following right-recursive
            rule accepts the same input, but during parsing, it
            requires linear stack space to read object field
            definitions:

	    <idx>Menhir parser generator/left-recursive definitions</idx></p>

          <link rel="import" href="code/parsing/right_rec_rule.mly" part="4"/>

          <p>
	    Alternatively, we could keep the left-recursive
            definition and simply construct the returned value in
            left-to-right order. This is even less efficient, since the
            complexity of building the list incrementally in this way
            is quadratic in the length of the list:</p>

          <link rel="import" href="code/parsing/quadratic_rule.mly" part="4"/>

          <p>
	    Assembling lists like this is a pretty common
            requirement in most realistic grammars, and the preceding
            rules (while useful for illustrating how parsing works) are
            rather verbose. Menhir features an extended standard
            library of built-in rules to simplify this handling. These
            rules are detailed in the Menhir manual and include
            optional values, pairs of values with optional separators,
            and lists of elements (also with optional separators).

	    <idx>Menhir parser generator/built-in rules of</idx></p>

          <p>
	    A version of the JSON grammar using these more succinct
            Menhir rules follows. Notice the use of
            <code>separated_list</code> to parse both JSON objects and
            lists with one rule:</p>
          <link rel="import" href="code/parsing/short_parser.mly" part="1"/>

          <p>
	    We can invoke <span class="command"><em>menhir</em></span>
            by using <span class="command"><em>corebuild</em></span>
            with the <code>-use-menhir</code> flag. This tells the
            build system to switch to
            using <span class="command"><em>menhir</em></span> instead
            of <span class="command"><em>ocamlyacc</em></span> to
            handle files with the <code>.mly</code> suffix:

	    <idx data-primary-sortas="use">-use-menhir flag</idx>
	    <idx>Menhir parser generator/invoking</idx>
	    <a data-type="indexterm" data-startref="PARSparsdef">&nbsp;</a></p>

          <link rel="import" href="code/parsing/jbuild"/>
          <link rel="import" href="code/parsing/build_short_parser.sh"/>
	</section>
      </section>

      <section id="defining-a-lexer" data-type="sect1">
	<h1>Defining a Lexer</h1>

	<p>
	  Now we can define a lexer,
	  using <span class="command"><em>ocamllex</em></span>, to
	  convert our input text into a stream of tokens. The
	  specification of the lexer is placed in a file with
	  an <code>.mll</code> suffix.

	  <idx>lexers/specification of</idx>
	  <idx>OCaml toolchain/ocamllex</idx>
	  <idx>mll files</idx>
	  <idx>files/mll files</idx>
	  <idx id="PARlex">parsing/lexer definition</idx></p>

	<section id="ocaml-prelude" data-type="sect2">
          <h2>OCaml Prelude</h2>

          <p>
	    Let's walk through the definition of a lexer section by
            section. The first section is an optional chunk of OCaml
            code that is bounded by a pair of curly braces:

	    <idx>lexers/optional OCaml code for</idx></p>
          <link rel="import" href="code/parsing/lexer.mll"/>

          <p>
	    This code is there to define utility functions used by
            later snippets of OCaml code and to set up the environment
            by opening useful modules and define an exception,
            <code>SyntaxError</code>.</p>

          <p>
	    We also define a utility function <code>next_line</code>
            for tracking the location of tokens across line breaks. The
            <code>Lexing</code> module defines a <code>lexbuf</code>
            structure that holds the state of the lexer, including the
            current location within the source file. The
            <code>next_line</code> function simply accesses the
            <code>lex_curr_p</code> field that holds the current
            location and updates its line number.</p>
	</section>

	<section id="regular-expressions" data-type="sect2">
          <h2>Regular Expressions</h2>

          <p>
	    The next section of the lexing file is a collection of
            named regular expressions. These look syntactically like
            ordinary OCaml <code>let</code> bindings, but really this
            is a specialized syntax for declaring regular expressions.
            Here's an example:

	    <idx>regular expressions</idx>
	    <idx>lexers/regular expressions collection</idx></p>

          <link rel="import" href="code/parsing/lexer.mll" part="1"/>

          <p>
	    The syntax here is something of a hybrid between OCaml
            syntax and traditional regular expression syntax. The
            <code>int</code> regular expression specifies an optional
            leading <code>-</code>, followed by a digit from
            <code>0</code> to <code>9</code>, followed by some number
            of digits from <code>0</code> to <code>9</code>. The
            question mark is used to indicate an optional component of
            a regular expression; the square brackets are used to
            specify ranges; and the <code>*</code> operator is used to
            indicate a (possibly empty) repetition.</p>

          <p>
	    Floating-point numbers are specified similarly, but we
            deal with decimal points and exponents. We make the
            expression easier to read by building up a sequence of
            named regular expressions, rather than creating one big and
            impenetrable expression:</p>
          <link rel="import" href="code/parsing/lexer.mll" part="2"/>

          <p>
	    Finally, we define whitespace, newlines, and
            identifiers:</p>

          <link rel="import" href="code/parsing/lexer.mll" part="3"/>

          <p>
	    The <code>newline</code> introduces the <code>|</code>
            operator, which lets one of several alternative regular
            expressions match (in this case, the various
            carriage-return combinations of CR, LF, or CRLF).</p>
	</section>

	<section id="lexing-rules" data-type="sect2">
          <h2>Lexing Rules</h2>

          <p>
	    The lexing rules are essentially functions that consume
            the data, producing OCaml expressions that evaluate to
            tokens. These OCaml expressions can be quite complicated,
            using side effects and invoking other rules as part of the
            body of the rule. Let's look at the <code>read</code> rule
            for parsing a JSON expression:

	    <idx>lexers/rules for</idx></p>

          <link rel="import" href="code/parsing/lexer.mll" part="4"/>

          <p>
	    The rules are structured very similarly to pattern
            matches, except that the variants are replaced by regular
            expressions on the left-hand side. The righthand-side clause
            is the parsed OCaml return value of that rule. The OCaml
            code for the rules has a parameter called
            <code>lexbuf</code> that defines the input, including the
            position in the input file, as well as the text that was
            matched by the regular expression.

	    <idx>pattern matching/vs. lexing rules</idx></p>

          <p>
	    The first <code>white { read lexbuf }</code> calls the
            lexer recursively. That is, it skips the input whitespace
            and returns the following token. The action <code>newline {
              next_line lexbuf; read lexbuf }</code> is similar, but we
            use it to advance the line number for the lexer using the
            utility function that we defined at the top of the file.
            Let's skip to the third action:</p>

          <link rel="import" href="code/parsing/lexer_int_fragment.mll"/>

          <p>
	    This action specifies that when the input matches the
            <code>int</code> regular expression, then the lexer should
            return the expression <code>INT (int_of_string
              (Lexing.lexeme lexbuf))</code>. The expression
            <code>Lexing.lexeme lexbuf</code> returns the complete
            string matched by the regular expression. In this case, the
            string represents a number, so we use the
            <code>int_of_string</code> function to convert it to a
            number.</p>

          <p>
	    There are actions for each different kind of token. The
            string expressions like <code>"true" { TRUE }</code> are
            used for keywords, and the special characters have actions,
            too, like <code>'{' { LEFT_BRACE }</code>.</p>

          <p>
	    Some of these patterns overlap. For example, the regular
            expression <code>"true"</code> is also matched by
            the <code>id</code>
            pattern. <span class="command"><em>ocamllex</em></span>
            used the following disambiguation when a prefix of the
            input is matched by more than one pattern:</p>

          <ul>
            <li>
              <p>
		The longest match always wins. For example, the
		first input <code>trueX: 167</code> matches the regular
		expression <code>"true"</code> for four characters, and
		it matches <code>id</code> for five characters. The
		longer match wins, and the return value is <code>ID
		  "trueX"</code>.</p>
            </li>

            <li>
              <p>
		If all matches have the same length, then the first
		action wins. If the input were <code>true: 167</code>,
		then both <code>"true"</code> and <code>id</code> match
		the first four characters; <code>"true"</code> is
		first, so the return value is <code>TRUE</code>.</p>
            </li>
          </ul>
	</section>

	<section id="recursive-rules" data-type="sect2">
          <h2>Recursive Rules</h2>

          <p>
	    Unlike many other lexer
	    generators, <span class="command"><em>ocamllex</em></span>
	    allows the definition of multiple lexers in the same file,
	    and the definitions can be recursive. In this case, we use
	    recursion to match string literals using the following
	    rule definition:

	    <idx>recursion/in lexers</idx>
	    <idx>lexers/recursive rules</idx></p>

          <link rel="import" href="code/parsing/lexer.mll" part="5"/>

          <p>
	    This rule takes a <code>buf : Buffer.t</code> as an
            argument. If we reach the terminating double quote
            <code>"</code>, then we return the contents of the buffer
            as a <code>STRING</code>.</p>

          <p>
	    The other cases are for handling the string contents.
            The action <code>[^ '"' '\\']+ { ... }</code> matches
            normal input that does not contain a double quote or
            backslash. The actions beginning with a backslash
            <code>\</code> define what to do for escape sequences. In
            each of these cases, the final step includes a recursive
            call to the lexer.</p>

          <p>
	    That covers the lexer. Next, we need to combine the
            lexer with the parser to bring it all together.

	    <idx>lexers/Unicode parsing</idx>
	    <idx>Uutf Unicode codec</idx>
	    <idx>OCaml toolchain/ocamllex</idx>
	    <idx>Ulex lexer generator</idx>
	    <idx>Camomile unicode parser</idx>
	    <idx>Unicode, parsing solutions for</idx></p>

          <div data-type="note">
            <h1>Handling Unicode</h1>

            <p>
	      We've glossed over an important detail here: parsing
              Unicode characters to handle the full spectrum of the
              world's writing systems. OCaml has several third-party
              solutions to handling Unicode, with varying degrees of
              flexibility and complexity:</p>

            <ul>
              <li>
		<p>
		  <a href="http://camomile.sourceforge.net">Camomile</a>
		  supports the full spectrum of Unicode character
		  types, conversion from around 200 encodings, and
		  collation and locale-sensitive case mappings.</p>
              </li>

              <li>
		<p>
		  <a href="http://www.cduce.org/ulex">Ulex</a> is a
		  lexer generator for Unicode that can serve as a
		  Unicode-aware replacement
		  for <span class="command"><em>ocamllex</em></span>.</p>
              </li>

              <li>
		<p>
		  <a href="http://erratique.ch/software/uutf">Uutf</a> is a
		  nonblocking streaming Unicode codec for OCaml,
		  available as a standalone library. It is accompanied
		  by the <a href="http://erratique.ch/software/uunf">Uunf</a> text
		  normalization and <a href="http://erratique.ch/software/uucd">Uucd</a> Unicode
		  character database libraries. There is also a robust
		  parser for <a href="http://erratique.ch/software/jsonm">JSON</a>
		  available that illustrates the use of Uutf in your
		  own libraries.</p>
              </li>
            </ul>

            <p>
	      All of these libraries are available via OPAM under
              their respective names.<a data-type="indexterm" data-startref="PARlex">&nbsp;</a></p>
          </div>
	</section>
      </section>

      <section id="bringing-it-all-together" data-type="sect1">
	<h1>Bringing It All Together</h1>

	<p>
	  For the final part, we need to compose the lexer and
	  parser. As we saw in the type definition in
	  <code>parser.mli</code>, the parsing function expects a lexer
	  of type <code>Lexing.lexbuf -&gt; token</code>, and a
	  <code>lexbuf</code>:

	  <idx>parsing/lexer and parser composition</idx></p>


	<link rel="import" href="code/parsing/prog.mli"/>

	<p>
	  Before we start with the lexing, let's first define some
	  functions to handle parsing errors. There are currently two
	  errors: <code>Parser.Error</code> and
	  <code>Lexer.SyntaxError</code>. A simple solution when
	  encountering an error is to print the error and give up:

	  <idx>errors/&quot;give up on first error&quot; approach</idx></p>
	<link rel="import" href="code/parsing-test/test.ml"/>

	<p>
	  The "give up on the first error" approach is easy to
	  implement but isn't very friendly. In general, error
	  handling can be pretty intricate, and we won't discuss it
	  here.  However, the Menhir parser defines additional
	  mechanisms you can use to try and recover from errors. These
	  are described in detail in its
	  reference <a href="http://gallium.inria.fr/~fpottier/menhir/">manual</a>.

	  <idx>Menhir parser generator/error handling in</idx></p>

	<p>
	  The standard lexing library <code>Lexing</code> provides a
	  function <code>from_channel</code> to read the input from a
	  channel. The following function describes the structure,
	  where the <code>Lexing.from_channel</code> function is used
	  to construct a <code>lexbuf</code>, which is passed with the
	  lexing function <code>Lexer.read</code> to the
	  <code>Parser.prog</code> function. <code>Parsing.prog</code>
	  returns <code>None</code> when it reaches end of file. We
	  define a function <code>Json.output_value</code>, not shown
	  here, to print a <code>Json.value</code>:</p>

	<link rel="import" href="code/parsing-test/test.ml" part="1"/>

	<p>
	  Here's a test input file we can use to test the code we
	  just wrote:</p>

	<link rel="import" href="code/parsing-test/test1.json"/>

	<p>
	  Now build and run the example using this file, and you can
	  see the full parser in action:</p>

	<link rel="import" href="code/parsing-test/build_test.sh"/>

	<p>
	  With our simple error handling scheme, errors are fatal
	  and cause the program to terminate with a nonzero exit
	  code:</p>

	<link rel="import" href="code/parsing-test/run_broken_test.errsh"/>

	<p>
	  That wraps up our parsing tutorial. As an aside, notice that
	  the JSON polymorphic variant type that we defined in this
	  chapter is actually structurally compatible with the Yojson
	  representation explained
	  in <a href="15-json.html#handling-json-data" data-type="xref">Handling
	    Json Data</a>. That means that you can take this parser and
	  use it with the helper functions in Yojson to build more
	  sophisticated applications.</p>
      </section>
    </section>
